import streamlit as st
from openai import OpenAI
from PIL import Image
import requests
from io import BytesIO
import datetime
import base64
from typing import Dict, List, Optional, Tuple
import time
import random
import json
import sqlite3
import uuid
import zipfile
import psutil
import os
import re
from urllib.parse import urlencode, quote

# å…¼å®¹æ€§å‡½æ•¸
def rerun_app():
    """å…¼å®¹ä¸åŒ Streamlit ç‰ˆæœ¬çš„é‡æ–°é‹è¡Œå‡½æ•¸"""
    if hasattr(st, 'rerun'):
        st.rerun()
    elif hasattr(st, 'experimental_rerun'):
        st.experimental_rerun()
    else:
        st.stop()

# è¨­å®šé é¢é…ç½®
st.set_page_config(
    page_title="Flux AI åœ–åƒç”Ÿæˆå™¨ Pro - å®Œæ•´ç‰ˆ",
    page_icon="ğŸ¨",
    layout="wide"
)

# API æä¾›å•†é…ç½®
API_PROVIDERS = {
    "OpenAI Compatible": {
        "name": "OpenAI Compatible API",
        "base_url_default": "https://api.openai.com/v1",
        "key_prefix": "sk-",
        "description": "OpenAI å®˜æ–¹æˆ–å…¼å®¹çš„ API æœå‹™",
        "icon": "ğŸ¤–"
    },
    "Navy": {
        "name": "Navy API",
        "base_url_default": "https://api.navy/v1",
        "key_prefix": "sk-",
        "description": "Navy æä¾›çš„ AI åœ–åƒç”Ÿæˆæœå‹™",
        "icon": "âš“"
    },
    "Pollinations.ai": {
        "name": "Pollinations.ai",
        "base_url_default": "https://image.pollinations.ai",
        "key_prefix": "",
        "description": "å…è²»ã€é–‹æºçš„åœ–åƒç”Ÿæˆ API (ç„¡éœ€å¯†é‘°)",
        "icon": "ğŸŒ¸"
    },
    "Hugging Face": {
        "name": "Hugging Face Inference",
        "base_url_default": "https://api-inference.huggingface.co",
        "key_prefix": "hf_",
        "description": "Hugging Face Inference API",
        "icon": "ğŸ¤—"
    },
    "Together AI": {
        "name": "Together AI",
        "base_url_default": "https://api.together.xyz/v1",
        "key_prefix": "",
        "description": "Together AI å¹³å°",
        "icon": "ğŸ¤"
    },
    "Fireworks AI": {
        "name": "Fireworks AI",
        "base_url_default": "https://api.fireworks.ai/inference/v1",
        "key_prefix": "",
        "description": "Fireworks AI å¿«é€Ÿæ¨ç†",
        "icon": "ğŸ†"
    },
    "Custom": {
        "name": "è‡ªå®šç¾© API",
        "base_url_default": "",
        "key_prefix": "",
        "description": "è‡ªå®šç¾©çš„ API ç«¯é»",
        "icon": "ğŸ”§"
    }
}

# åŸºç¤ Flux æ¨¡å‹é…ç½®
BASE_FLUX_MODELS = {
    "flux.1-schnell": {
        "name": "FLUX.1 Schnell",
        "description": "æœ€å¿«çš„ç”Ÿæˆé€Ÿåº¦ï¼Œé–‹æºæ¨¡å‹",
        "icon": "âš¡",
        "type": "å¿«é€Ÿç”Ÿæˆ",
        "test_prompt": "A simple cat sitting on a table",
        "expected_size": "1024x1024",
        "priority": 1,
        "source": "base"
    },
    "flux.1-dev": {
        "name": "FLUX.1 Dev",
        "description": "é–‹ç™¼ç‰ˆæœ¬ï¼Œå¹³è¡¡é€Ÿåº¦èˆ‡è³ªé‡",
        "icon": "ğŸ”§",
        "type": "é–‹ç™¼ç‰ˆæœ¬",
        "test_prompt": "A beautiful landscape with mountains",
        "expected_size": "1024x1024",
        "priority": 2,
        "source": "base"
    },
    "flux.1.1-pro": {
        "name": "FLUX.1.1 Pro",
        "description": "æ”¹é€²çš„æ——è‰¦æ¨¡å‹ï¼Œæœ€ä½³å“è³ª",
        "icon": "ğŸ‘‘",
        "type": "æ——è‰¦ç‰ˆæœ¬",
        "test_prompt": "Professional portrait of a person in business attire",
        "expected_size": "1024x1024",
        "priority": 3,
        "source": "base"
    },
    "flux.1-kontext-pro": {
        "name": "FLUX.1 Kontext Pro",
        "description": "æ”¯æŒåœ–åƒç·¨è¼¯å’Œä¸Šä¸‹æ–‡ç†è§£",
        "icon": "ğŸ¯",
        "type": "ç·¨è¼¯å°ˆç”¨",
        "test_prompt": "Abstract geometric shapes in vibrant colors",
        "expected_size": "1024x1024",
        "priority": 4,
        "source": "base"
    }
}

# æ¨¡å‹è‡ªå‹•ç™¼ç¾è¦å‰‡
FLUX_MODEL_PATTERNS = {
    r'flux[\.\-]?1[\.\-]?schnell': {
        "name_template": "FLUX.1 Schnell",
        "icon": "âš¡",
        "type": "å¿«é€Ÿç”Ÿæˆ",
        "priority_base": 100
    },
    r'flux[\.\-]?1[\.\-]?dev': {
        "name_template": "FLUX.1 Dev",
        "icon": "ğŸ”§",
        "type": "é–‹ç™¼ç‰ˆæœ¬",
        "priority_base": 200
    },
    r'flux[\.\-]?1[\.\-]?pro': {
        "name_template": "FLUX.1 Pro",
        "icon": "ğŸ‘‘",
        "type": "å°ˆæ¥­ç‰ˆæœ¬",
        "priority_base": 300
    },
    r'flux[\.\-]?1[\.\-]?kontext': {
        "name_template": "FLUX.1 Kontext",
        "icon": "ğŸ¯",
        "type": "ä¸Šä¸‹æ–‡ç†è§£",
        "priority_base": 400
    }
}

# æä¾›å•†ç‰¹å®šçš„æ¨¡å‹ç«¯é»
HF_FLUX_ENDPOINTS = [
    "black-forest-labs/FLUX.1-schnell",
    "black-forest-labs/FLUX.1-dev",
    "black-forest-labs/FLUX.1.1-pro",
]

def auto_discover_flux_models(client, provider: str, api_key: str, base_url: str) -> Dict[str, Dict]:
    """è‡ªå‹•ç™¼ç¾æ¨¡å‹ï¼Œç¾å·²æ”¯æŒ Pollinations.ai"""
    discovered_models = {}
    
    try:
        if provider == "Pollinations.ai":
            models_url = f"{base_url}/models"
            response = requests.get(models_url, timeout=10)
            if response.status_code == 200:
                models_list = response.json()
                for model_name in models_list:
                    model_id = model_name
                    model_info = analyze_model_name(model_id)
                    model_info['source'] = 'pollinations'
                    model_info['type'] = 'åœ–åƒå°ˆç”¨'
                    model_info['icon'] = 'ğŸŒ¸'
                    discovered_models[model_id] = model_info
            else:
                st.warning(f"ç„¡æ³•å¾ Pollinations.ai ç²å–æ¨¡å‹åˆ—è¡¨ (HTTP {response.status_code})")

        elif provider == "Hugging Face":
            for endpoint in HF_FLUX_ENDPOINTS:
                model_id = endpoint.split('/')[-1]
                model_info = analyze_model_name(model_id, endpoint)
                model_info['source'] = 'huggingface'
                model_info['endpoint'] = endpoint
                discovered_models[model_id] = model_info
        else:
            response = client.models.list()
            for model in response.data:
                model_id = model.id
                # é™åˆ¶åªé¡¯ç¤ºåŒ…å« 'flux' çš„åœ–åƒæ¨¡å‹
                if is_flux_model(model_id):
                    model_info = analyze_model_name(model.id)
                    model_info['source'] = 'api_discovery'
                    discovered_models[model.id] = model_info

        return discovered_models
    except Exception as e:
        st.warning(f"æ¨¡å‹è‡ªå‹•ç™¼ç¾å¤±æ•—: {str(e)}")
        return {}

def is_flux_model(model_name: str) -> bool:
    """æª¢æŸ¥æ¨¡å‹åç¨±æ˜¯å¦ç‚º Flux æ¨¡å‹"""
    model_lower = model_name.lower()
    flux_keywords = ['flux', 'black-forest-labs']
    return any(keyword in model_lower for keyword in flux_keywords)

def analyze_model_name(model_id: str, full_path: str = None) -> Dict:
    """åˆ†ææ¨¡å‹åç¨±ä¸¦ç”Ÿæˆæ¨¡å‹ä¿¡æ¯"""
    model_lower = model_id.lower()
    
    for pattern, info in FLUX_MODEL_PATTERNS.items():
        if re.search(pattern, model_lower):
            analyzed_info = {
                "name": info["name_template"],
                "icon": info["icon"],
                "type": info["type"],
                "description": f"è‡ªå‹•ç™¼ç¾çš„ {info['name_template']} æ¨¡å‹",
                "test_prompt": "A beautiful scene with detailed elements",
                "expected_size": "1024x1024",
                "priority": info["priority_base"] + hash(model_id) % 100,
                "auto_discovered": True
            }
            
            if full_path:
                analyzed_info["full_path"] = full_path
                if '/' in full_path:
                    author = full_path.split('/')[0]
                    analyzed_info["name"] += f" ({author})"
            
            return analyzed_info
    
    return {
        "name": model_id.replace('-', ' ').replace('_', ' ').title(),
        "icon": "ğŸ¤–",
        "type": "è‡ªå‹•ç™¼ç¾",
        "description": f"è‡ªå‹•ç™¼ç¾çš„æ¨¡å‹: {model_id}",
        "test_prompt": "A detailed and beautiful image",
        "expected_size": "1024x1024",
        "priority": 999,
        "auto_discovered": True,
        "full_path": full_path if full_path else model_id
    }

def merge_models() -> Dict[str, Dict]:
    """åˆä½µåŸºç¤æ¨¡å‹å’Œè‡ªå‹•ç™¼ç¾çš„æ¨¡å‹"""
    discovered = st.session_state.get('discovered_models', {})
    merged_models = BASE_FLUX_MODELS.copy()
    
    for model_id, model_info in discovered.items():
        if model_id not in merged_models:
            merged_models[model_id] = model_info
            
    # æŒ‰ 'priority' æ’åº
    sorted_models = sorted(merged_models.items(), key=lambda item: item[1].get('priority', 999))
    return dict(sorted_models)


def validate_api_key(api_key: str, base_url: str, provider: str) -> Tuple[bool, str]:
    """é©—è­‰ API å¯†é‘°æ˜¯å¦æœ‰æ•ˆï¼Œæ–°å¢ Pollinations.ai é©—è­‰"""
    try:
        if provider == "Pollinations.ai":
            test_url = f"{base_url}/models"
            response = requests.get(test_url, timeout=10)
            if response.status_code == 200:
                return True, "Pollinations.ai æœå‹™é€£æ¥æˆåŠŸ"
            else:
                return False, f"HTTP {response.status_code}: Pollinations.ai é€£æ¥å¤±æ•—"

        elif provider == "Hugging Face":
            headers = {"Authorization": f"Bearer {api_key}"}
            test_url = f"{base_url}/models/black-forest-labs/FLUX.1-schnell"
            response = requests.get(test_url, headers=headers, timeout=10)
            if response.status_code == 200:
                return True, "Hugging Face API å¯†é‘°é©—è­‰æˆåŠŸ"
            else:
                return False, f"HTTP {response.status_code}: é©—è­‰å¤±æ•—"
        else:
            test_client = OpenAI(api_key=api_key, base_url=base_url)
            response = test_client.models.list()
            return True, "API å¯†é‘°é©—è­‰æˆåŠŸ"
    except Exception as e:
        error_msg = str(e)
        if "401" in error_msg or "Unauthorized" in error_msg:
            return False, "API å¯†é‘°ç„¡æ•ˆæˆ–å·²éæœŸ"
        elif "403" in error_msg or "Forbidden" in error_msg:
            return False, "API å¯†é‘°æ²’æœ‰è¶³å¤ æ¬Šé™"
        elif "404" in error_msg:
            return False, "API ç«¯é»ä¸å­˜åœ¨æˆ–ä¸æ­£ç¢º"
        elif "timeout" in error_msg.lower():
            return False, "API é€£æ¥è¶…æ™‚"
        else:
            return False, f"API é©—è­‰å¤±æ•—: {error_msg[:100]}"

def test_model_availability(client, model_name: str, provider: str, api_key: str, base_url: str, test_prompt: str = None) -> Dict:
    """æ¸¬è©¦ç‰¹å®šæ¨¡å‹çš„å¯ç”¨æ€§"""
    all_models = merge_models()
    if test_prompt is None:
        test_prompt = all_models.get(model_name, {}).get('test_prompt', 'A simple test image')
    
    test_result = {
        'model': model_name,
        'available': False,
        'response_time': 0,
        'error': None,
        'details': {}
    }
    
    try:
        start_time = time.time()
        
        if provider == "Hugging Face":
            headers = {"Authorization": f"Bearer {api_key}"}
            data = {"inputs": test_prompt}
            
            model_info = all_models.get(model_name, {})
            endpoint_path = model_info.get('full_path', f"black-forest-labs/{model_name}")
            
            response = requests.post(
                f"{base_url}/models/{endpoint_path}",
                headers=headers,
                json=data,
                timeout=30
            )
            end_time = time.time()
            response_time = end_time - start_time
            
            if response.status_code == 200:
                test_result.update({
                    'available': True,
                    'response_time': response_time,
                    'details': {
                        'status_code': response.status_code,
                        'test_prompt': test_prompt,
                        'endpoint': endpoint_path
                    }
                })
            else:
                test_result.update({
                    'available': False,
                    'error': f"HTTP {response.status_code}",
                    'details': {'status_code': response.status_code}
                })
        else:
            response = client.images.generate(
                model=model_name,
                prompt=test_prompt,
                n=1,
                size="1024x1024"
            )
            end_time = time.time()
            response_time = end_time - start_time
            
            test_result.update({
                'available': True,
                'response_time': response_time,
                'details': {
                    'image_count': len(response.data),
                    'test_prompt': test_prompt,
                    'image_url': response.data[0].url if response.data else None
                }
            })
            
    except Exception as e:
        error_msg = str(e)
        test_result.update({
            'available': False,
            'error': error_msg,
            'details': {
                'error_type': 'generation_failed',
                'test_prompt': test_prompt
            }
        })
    
    return test_result

def generate_images_with_retry(client, provider: str, api_key: str, base_url: str, **params) -> Tuple[bool, any]:
    """å¸¶é‡è©¦æ©Ÿåˆ¶çš„åœ–åƒç”Ÿæˆï¼Œæ–°å¢ Pollinations.ai æ”¯æŒ"""
    max_retries = 3
    base_delay = 2
    
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                st.info(f"ğŸ”„ å˜—è©¦é‡æ–°ç”Ÿæˆ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)")

            if provider == "Pollinations.ai":
                # Pollinations.ai GET è«‹æ±‚
                prompt = params.get("prompt", "")
                width, height = params.get("size", "1024x1024").split('x')
                
                query_params = {
                    "model": params.get("model"),
                    "width": width,
                    "height": height,
                    "seed": random.randint(0, 1000000), # Pollinations uses seed
                    "nologo": "true"
                }
                
                # æ¸…ç† None å€¼
                query_params = {k: v for k, v in query_params.items() if v is not None}
                
                encoded_prompt = quote(prompt)
                request_url = f"{base_url}/prompt/{encoded_prompt}?{urlencode(query_params)}"
                
                response = requests.get(request_url, timeout=120)

                if response.status_code == 200:
                    # æ¨¡æ“¬ OpenAI éŸ¿æ‡‰æ ¼å¼
                    class MockResponse:
                        def __init__(self, image_data):
                            self.data = [type('obj', (object,), {
                                'url': f"data:image/png;base64,{base64.b64encode(image_data).decode()}"
                            })()]
                    
                    # Pollinations.ai åªæ”¯æŒä¸€æ¬¡ç”Ÿæˆä¸€å¼µåœ–
                    num_images = params.get("n", 1)
                    if num_images > 1:
                        st.warning("Pollinations.ai æ¯æ¬¡åƒ…æ”¯æŒç”Ÿæˆä¸€å¼µåœ–åƒï¼Œå·²è‡ªå‹•è¨­ç‚º 1ã€‚")

                    return True, MockResponse(response.content)
                else:
                    raise Exception(f"HTTP {response.status_code}: {response.text}")
            
            elif provider == "Hugging Face":
                # Hugging Face API èª¿ç”¨
                headers = {"Authorization": f"Bearer {api_key}"}
                data = {"inputs": params.get("prompt", "")}
                
                model_name = params.get("model", "FLUX.1-schnell")
                all_models = merge_models()
                model_info = all_models.get(model_name, {})
                endpoint_path = model_info.get('full_path', f"black-forest-labs/{model_name}")
                
                response = requests.post(
                    f"{base_url}/models/{endpoint_path}",
                    headers=headers,
                    json=data,
                    timeout=60
                )
                
                if response.status_code == 200:
                    # æ¨¡æ“¬ OpenAI éŸ¿æ‡‰æ ¼å¼
                    class MockResponse:
                        def __init__(self, image_data):
                            self.data = [type('obj', (object,), {
                                'url': f"data:image/png;base64,{base64.b64encode(image_data).decode()}"
                            })()]
                    
                    return True, MockResponse(response.content)
                else:
                    raise Exception(f"HTTP {response.status_code}: {response.text}")
            else:
                # OpenAI Compatible API èª¿ç”¨
                response = client.images.generate(**params)
                return True, response
            
        except Exception as e:
            error_msg = str(e)
            if attempt < max_retries - 1:
                should_retry = False
                if any(code in error_msg for code in ["500", "502", "503", "429"]):
                    should_retry = True
                elif "timeout" in error_msg.lower():
                    should_retry = True
                
                if should_retry:
                    delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                    st.warning(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—ï¼Œ{delay:.1f} ç§’å¾Œé‡è©¦...")
                    time.sleep(delay)
                    continue
                else:
                    return False, error_msg
            else:
                return False, error_msg
    
    return False, "æ‰€æœ‰é‡è©¦å‡å¤±æ•—"

def init_session_state():
    """åˆå§‹åŒ–æœƒè©±ç‹€æ…‹"""
    if 'api_config' not in st.session_state:
        st.session_state.api_config = {
            'provider': 'Navy',
            'api_key': '',
            'base_url': 'https://api.navy/v1',
            'validated': False
        }
    
    if 'generation_history' not in st.session_state:
        st.session_state.generation_history = []
    
    if 'favorite_images' not in st.session_state:
        st.session_state.favorite_images = []
    
    if 'model_test_results' not in st.session_state:
        st.session_state.model_test_results = {}
    
    if 'discovered_models' not in st.session_state:
        st.session_state.discovered_models = {}

def add_to_history(prompt: str, model: str, images: List[str], metadata: Dict):
    """æ·»åŠ ç”Ÿæˆè¨˜éŒ„åˆ°æ­·å²"""
    history_item = {
        "timestamp": datetime.datetime.now(),
        "prompt": prompt,
        "model": model,
        "images": images,
        "metadata": metadata,
        "id": str(uuid.uuid4())
    }
    st.session_state.generation_history.insert(0, history_item)
    
    # é™åˆ¶æ­·å²è¨˜éŒ„æ•¸é‡
    if len(st.session_state.generation_history) > 50:
        st.session_state.generation_history = st.session_state.generation_history[:50]

def display_image_with_actions(image_url: str, image_id: str, history_item: Dict = None):
    """é¡¯ç¤ºåœ–åƒå’Œç›¸é—œæ“ä½œ"""
    try:
        # è™•ç† base64 åœ–åƒ
        if image_url.startswith('data:image'):
            base64_data = image_url.split(',')[1]
            img_data = base64.b64decode(base64_data)
            img = Image.open(BytesIO(img_data))
        else:
            img_response = requests.get(image_url, timeout=10)
            img = Image.open(BytesIO(img_response.content))
            img_data = img_response.content
        
        st.image(img, use_column_width=True)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            img_buffer = BytesIO()
            img.save(img_buffer, format='PNG')
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰",
                data=img_buffer.getvalue(),
                file_name=f"flux_generated_{image_id}.png",
                mime="image/png",
                key=f"download_{image
