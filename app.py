import streamlit as st
import requests
from PIL import Image
from io import BytesIO
import time
import os
import psutil
import sys

# é é¢é…ç½®
st.set_page_config(
    page_title="Flux AI - Koyeb CPU",
    page_icon="ğŸš€",
    layout="wide"
)

# Koyeb å°ˆç”¨ CSS
st.markdown("""
<style>
.koyeb-header {
    background: linear-gradient(90deg, #2563eb 0%, #1d4ed8 100%);
    padding: 2rem;
    border-radius: 10px;
    color: white;
    text-align: center;
    margin-bottom: 2rem;
}

.resource-monitor {
    background: #f8fafc;
    padding: 1rem;
    border-radius: 8px;
    border-left: 4px solid #2563eb;
    margin: 1rem 0;
}

.koyeb-stats {
    position: fixed;
    top: 70px;
    right: 20px;
    background: rgba(255,255,255,0.95);
    padding: 10px;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    font-size: 0.8rem;
    z-index: 1000;
}

.api-card {
    background: white;
    padding: 1rem;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    margin: 0.5rem 0;
}
</style>
""", unsafe_allow_html=True)

# API æœå‹™é…ç½®
API_SERVICES = {
    "Hugging Face Inference": {
        "endpoint": "https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell",
        "free_quota": "1000 è«‹æ±‚/æœˆ",
        "avg_time": "10-20ç§’",
        "quality": "é«˜å“è³ª"
    },
    "FAL.AI": {
        "endpoint": "https://fal.run/fal-ai/flux/schnell", 
        "free_quota": "50 å¼µ/æœˆ",
        "avg_time": "5-10ç§’",
        "quality": "é«˜å“è³ª"
    },
    "Replicate": {
        "model": "black-forest-labs/flux-schnell",
        "free_quota": "æœ‰é™è©¦ç”¨",
        "avg_time": "15-30ç§’", 
        "quality": "æœ€é«˜å“è³ª"
    }
}

def get_system_info():
    """ç²å– Koyeb ç³»çµ±è³‡æºä¿¡æ¯"""
    try:
        # CPU ä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # å…§å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        memory_used_mb = memory.used / (1024**2)
        memory_total_mb = memory.total / (1024**2)
        memory_percent = memory.percent
        
        # ç£ç›¤ä½¿ç”¨
        disk = psutil.disk_usage('/')
        disk_used_gb = disk.used / (1024**3)
        disk_total_gb = disk.total / (1024**3)
        
        return {
            "cpu_percent": cpu_percent,
            "memory_used": memory_used_mb,
            "memory_total": memory_total_mb,
            "memory_percent": memory_percent,
            "disk_used": disk_used_gb,
            "disk_total": disk_total_gb,
            "python_version": sys.version.split()[0]
        }
    except Exception as e:
        return {"error": str(e)}

def call_huggingface_api(prompt, hf_token):
    """èª¿ç”¨ Hugging Face Inference API"""
    headers = {
        "Authorization": f"Bearer {hf_token}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "inputs": prompt,
        "parameters": {
            "guidance_scale": 0.0,
            "num_inference_steps": 4
        }
    }
    
    try:
        response = requests.post(
            API_SERVICES["Hugging Face Inference"]["endpoint"],
            headers=headers,
            json=payload,
            timeout=60
        )
        
        if response.status_code == 200:
            image = Image.open(BytesIO(response.content))
            return {"status": "success", "image": image, "service": "Hugging Face"}
        else:
            return {"status": "error", "message": f"HTTP {response.status_code}: {response.text}"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

def call_replicate_api(prompt, replicate_token):
    """èª¿ç”¨ Replicate API"""
    try:
        import replicate
        
        os.environ["REPLICATE_API_TOKEN"] = replicate_token
        
        output = replicate.run(
            "black-forest-labs/flux-schnell",
            input={
                "prompt": prompt,
                "num_outputs": 1,
                "aspect_ratio": "1:1",
                "output_format": "webp",
                "output_quality": 80
            }
        )
        
        # ä¸‹è¼‰åœ–åƒ
        image_url = output[0] if isinstance(output, list) else output
        response = requests.get(image_url, timeout=30)
        image = Image.open(BytesIO(response.content))
        
        return {"status": "success", "image": image, "service": "Replicate"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

def simulate_demo_generation(prompt):
    """æ¼”ç¤ºæ¨¡å¼ - ç”Ÿæˆä½”ä½ç¬¦åœ–åƒ"""
    try:
        # å‰µå»ºå¸¶æœ‰æç¤ºè©çš„ä½”ä½ç¬¦åœ–åƒ
        placeholder_text = prompt[:30] + "..." if len(prompt) > 30 else prompt
        placeholder_url = f"https://via.placeholder.com/512x512/2563eb/ffffff?text={placeholder_text.replace(' ', '+')}"
        
        response = requests.get(placeholder_url, timeout=10)
        image = Image.open(BytesIO(response.content))
        
        return {"status": "success", "image": image, "service": "Demo"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

def main():
    # ä¸»æ¨™é¡Œ
    st.markdown("""
    <div class="koyeb-header">
        <h1>ğŸš€ Flux AI on Koyeb CPU</h1>
        <p>é«˜æ€§èƒ½ CPU å¯¦ä¾‹ | è‡ªå‹•ç¸®æ”¾ | å…¨çƒéƒ¨ç½²</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ç³»çµ±è³‡æºç›£æ§
    system_info = get_system_info()
    
    if "error" not in system_info:
        st.markdown(f"""
        <div class="koyeb-stats">
            <strong>ğŸ–¥ï¸ Koyeb å¯¦ä¾‹ç‹€æ…‹</strong><br>
            CPU: {system_info['cpu_percent']:.1f}%<br>
            RAM: {system_info['memory_used']:.0f}MB / {system_info['memory_total']:.0f}MB<br>
            ç£ç›¤: {system_info['disk_used']:.1f}GB / {system_info['disk_total']:.1f}GB<br>
            Python: {system_info['python_version']}
        </div>
        """, unsafe_allow_html=True)
    
    # å´é‚Šæ¬„é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ Koyeb é…ç½®")
        
        # å¯¦ä¾‹ä¿¡æ¯
        st.markdown("""
        <div class="resource-monitor">
            <h4>ğŸ“Š ç•¶å‰å¯¦ä¾‹</h4>
            <p><strong>é¡å‹:</strong> Free / Nano</p>
            <p><strong>vCPU:</strong> 0.1 - 0.25</p>
            <p><strong>RAM:</strong> 256-512MB</p>
            <p><strong>ç£ç›¤:</strong> 2-5GB SSD</p>
        </div>
        """, unsafe_allow_html=True)
        
        # API æœå‹™é¸æ“‡
        st.subheader("ğŸ”Œ API æœå‹™")
        
        selected_service = st.selectbox(
            "é¸æ“‡ç”Ÿæˆæœå‹™:",
            ["æ¼”ç¤ºæ¨¡å¼"] + list(API_SERVICES.keys())
        )
        
        if selected_service != "æ¼”ç¤ºæ¨¡å¼":
            # API Token è¼¸å…¥
            api_token = st.text_input(
                f"{selected_service} API Token:",
                type="password",
                help="è«‹åœ¨å®˜ç¶²ç²å–å…è²» API Token"
            )
            
            # æœå‹™ä¿¡æ¯
            if selected_service in API_SERVICES:
                service_info = API_SERVICES[selected_service]
                st.info(f"""
                **{selected_service}**
                - å…è²»é¡åº¦: {service_info['free_quota']}
                - å¹³å‡è€—æ™‚: {service_info['avg_time']}
                -
