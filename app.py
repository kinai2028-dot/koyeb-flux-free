import streamlit as st
import requests
from PIL import Image
from io import BytesIO
import time
import os
import psutil
import sys

# é é¢é…ç½®
st.set_page_config(
    page_title="Flux AI - Koyeb CPU",
    page_icon="ğŸš€",
    layout="wide"
)

# Koyeb å°ˆç”¨ CSS
st.markdown("""
<style>
.koyeb-header {
    background: linear-gradient(90deg, #2563eb 0%, #1d4ed8 100%);
    padding: 2rem;
    border-radius: 10px;
    color: white;
    text-align: center;
    margin-bottom: 2rem;
}

.resource-monitor {
    background: #f8fafc;
    padding: 1rem;
    border-radius: 8px;
    border-left: 4px solid #2563eb;
    margin: 1rem 0;
}

.koyeb-stats {
    position: fixed;
    top: 70px;
    right: 20px;
    background: rgba(255,255,255,0.95);
    padding: 10px;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    font-size: 0.8rem;
    z-index: 1000;
}

.api-card {
    background: white;
    padding: 1rem;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    margin: 0.5rem 0;
}
</style>
""", unsafe_allow_html=True)

# API æœå‹™é…ç½®
API_SERVICES = {
    "Hugging Face Inference": {
        "endpoint": "https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell",
        "free_quota": "1000 è«‹æ±‚/æœˆ",
        "avg_time": "10-20ç§’",
        "quality": "é«˜å“è³ª"
    },
    "FAL.AI": {
        "endpoint": "https://fal.run/fal-ai/flux/schnell", 
        "free_quota": "50 å¼µ/æœˆ",
        "avg_time": "5-10ç§’",
        "quality": "é«˜å“è³ª"
    },
    "Replicate": {
        "model": "black-forest-labs/flux-schnell",
        "free_quota": "æœ‰é™è©¦ç”¨",
        "avg_time": "15-30ç§’", 
        "quality": "æœ€é«˜å“è³ª"
    }
}

def get_system_info():
    """ç²å– Koyeb ç³»çµ±è³‡æºä¿¡æ¯"""
    try:
        # CPU ä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # å…§å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        memory_used_mb = memory.used / (1024**2)
        memory_total_mb = memory.total / (1024**2)
        memory_percent = memory.percent
        
        # ç£ç›¤ä½¿ç”¨
        disk = psutil.disk_usage('/')
        disk_used_gb = disk.used / (1024**3)
        disk_total_gb = disk.total / (1024**3)
        
        return {
            "cpu_percent": cpu_percent,
            "memory_used": memory_used_mb,
            "memory_total": memory_total_mb,
            "memory_percent": memory_percent,
            "disk_used": disk_used_gb,
            "disk_total": disk_total_gb,
            "python_version": sys.version.split()[0]
        }
    except Exception as e:
        return {"error": str(e)}

def call_huggingface_api(prompt, hf_token):
    """èª¿ç”¨ Hugging Face Inference API"""
    headers = {
        "Authorization": f"Bearer {hf_token}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "inputs": prompt,
        "parameters": {
            "guidance_scale": 0.0,
            "num_inference_steps": 4
        }
    }
    
    try:
        response = requests.post(
            API_SERVICES["Hugging Face Inference"]["endpoint"],
            headers=headers,
            json=payload,
            timeout=60
        )
        
        if response.status_code == 200:
            image = Image.open(BytesIO(response.content))
            return {"status": "success", "image": image, "service": "Hugging Face"}
        else:
            return {"status": "error", "message": f"HTTP {response.status_code}: {response.text}"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

def call_replicate_api(prompt, replicate_token):
    """èª¿ç”¨ Replicate API"""
    try:
        import replicate
        
        os.environ["REPLICATE_API_TOKEN"] = replicate_token
        
        output = replicate.run(
            "black-forest-labs/flux-schnell",
            input={
                "prompt": prompt,
                "num_outputs": 1,
                "aspect_ratio": "1:1",
                "output_format": "webp",
                "output_quality": 80
            }
        )
        
        # ä¸‹è¼‰åœ–åƒ
        image_url = output[0] if isinstance(output, list) else output
        response = requests.get(image_url, timeout=30)
        image = Image.open(BytesIO(response.content))
        
        return {"status": "success", "image": image, "service": "Replicate"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

def simulate_demo_generation(prompt):
    """æ¼”ç¤ºæ¨¡å¼ - ç”Ÿæˆä½”ä½ç¬¦åœ–åƒ"""
    try:
        # å‰µå»ºå¸¶æœ‰æç¤ºè©çš„ä½”ä½ç¬¦åœ–åƒ
        placeholder_text = prompt[:30] + "..." if len(prompt) > 30 else prompt
        placeholder_url = f"https://via.placeholder.com/512x512/2563eb/ffffff?text={placeholder_text.replace(' ', '+')}"
        
        response = requests.get(placeholder_url, timeout=10)
        image = Image.open(BytesIO(response.content))
        
        return {"status": "success", "image": image, "service": "Demo"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

def main():
    # ä¸»æ¨™é¡Œ
    st.markdown("""
    <div class="koyeb-header">
        <h1>ğŸš€ Flux AI on Koyeb CPU</h1>
        <p>é«˜æ€§èƒ½ CPU å¯¦ä¾‹ | è‡ªå‹•ç¸®æ”¾ | å…¨çƒéƒ¨ç½²</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ç³»çµ±è³‡æºç›£æ§
    system_info = get_system_info()
    
    if "error" not in system_info:
        st.markdown(f"""
        <div class="koyeb-stats">
            <strong>ğŸ–¥ï¸ Koyeb å¯¦ä¾‹ç‹€æ…‹</strong><br>
            CPU: {system_info['cpu_percent']:.1f}%<br>
            RAM: {system_info['memory_used']:.0f}MB / {system_info['memory_total']:.0f}MB<br>
            ç£ç›¤: {system_info['disk_used']:.1f}GB / {system_info['disk_total']:.1f}GB<br>
            Python: {system_info['python_version']}
        </div>
        """, unsafe_allow_html=True)
    
    # å´é‚Šæ¬„é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ Koyeb é…ç½®")
        
        # å¯¦ä¾‹ä¿¡æ¯
        st.markdown("""
        <div class="resource-monitor">
            <h4>ğŸ“Š ç•¶å‰å¯¦ä¾‹</h4>
            <p><strong>é¡å‹:</strong> Free / Nano</p>
            <p><strong>vCPU:</strong> 0.1 - 0.25</p>
            <p><strong>RAM:</strong> 256-512MB</p>
            <p><strong>ç£ç›¤:</strong> 2-5GB SSD</p>
        </div>
        """, unsafe_allow_html=True)
        
        # API æœå‹™é¸æ“‡
        st.subheader("ğŸ”Œ API æœå‹™")
        
        selected_service = st.selectbox(
            "é¸æ“‡ç”Ÿæˆæœå‹™:",
            ["æ¼”ç¤ºæ¨¡å¼"] + list(API_SERVICES.keys())
        )
        
        if selected_service != "æ¼”ç¤ºæ¨¡å¼":
            # API Token è¼¸å…¥
            api_token = st.text_input(
                f"{selected_service} API Token:",
                type="password",
                help="è«‹åœ¨å®˜ç¶²ç²å–å…è²» API Token"
            )
            
            # æœå‹™ä¿¡æ¯
            if selected_service in API_SERVICES:
                service_info = API_SERVICES[selected_service]
                st.info(f"""
                **{selected_service}**
                - å…è²»é¡åº¦: {service_info['free_quota']}
                - å¹³å‡è€—æ™‚: {service_info['avg_time']}
                - åœ–åƒå“è³ª: {service_info['quality']}
                """)
        else:
            api_token = None
            st.info("""
            **æ¼”ç¤ºæ¨¡å¼**
            - ç„¡ API æˆæœ¬
            - å³æ™‚éŸ¿æ‡‰
            - ä½”ä½ç¬¦åœ–åƒ
            - é©åˆæ¸¬è©¦éƒ¨ç½²
            """)
        
        st.divider()
        
        # å„ªåŒ–è¨­ç½®
        st.subheader("ğŸ›ï¸ æ€§èƒ½å„ªåŒ–")
        
        enable_cache = st.checkbox("å•Ÿç”¨çµæœç·©å­˜", value=True, help="æ¸›å°‘é‡è¤‡ API èª¿ç”¨")
        compress_images = st.checkbox("å£“ç¸®åœ–åƒ", value=True, help="æ¸›å°‘å…§å­˜ä½¿ç”¨")
        batch_processing = st.checkbox("æ‰¹æ¬¡è™•ç†", value=False, help="é©åˆå¤šå€‹è«‹æ±‚")
        
        # æˆæœ¬è¿½è¹¤
        st.subheader("ğŸ’° æˆæœ¬è¿½è¹¤")
        st.metric("Koyeb è²»ç”¨", "$0.00", "å…è²»é¡åº¦")
        st.metric("API æˆæœ¬", "è®Šå‹•", "ä¾ä½¿ç”¨é‡")
        st.metric("ç¸½é‹è¡Œæ™‚é–“", "24/7", "ä¸ä¼‘çœ ")
        
        # éƒ¨ç½²ä¿¡æ¯
        st.subheader("ğŸ“ éƒ¨ç½²ä¿¡æ¯")
        st.write("**å€åŸŸ**: è‡ªå‹•é¸æ“‡")
        st.write("**ç¸®æ”¾**: è‡ªå‹•")
        st.write("**SSL**: è‡ªå‹•")
        st.write("**åŸŸå**: .koyeb.app")
    
    # ä¸»ç•Œé¢
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“ AI åœ–åƒç”Ÿæˆ")
        
        # æç¤ºè©è¼¸å…¥
        prompt = st.text_area(
            "è¼¸å…¥æç¤ºè©:",
            placeholder="A beautiful mountain landscape with a serene lake",
            height=100,
            help="æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„åœ–åƒå…§å®¹"
        )
        
        # å¿«é€Ÿæç¤ºè©æ¨¡æ¿
        quick_prompts = {
            "è‡ªç„¶é¢¨æ™¯": "A serene mountain landscape with a crystal clear lake reflecting the sky",
            "ç¾ä»£å»ºç¯‰": "Modern glass skyscraper with sleek geometric design against blue sky", 
            "æŠ½è±¡è—è¡“": "Abstract geometric patterns with vibrant colors and flowing lines",
            "ç§‘æŠ€é¢¨æ ¼": "Futuristic digital interface with holographic elements and neon lights",
            "ç°¡ç´„è¨­è¨ˆ": "Minimalist design with clean lines and neutral color palette"
        }
        
        selected_template = st.selectbox("æˆ–é¸æ“‡å¿«é€Ÿæ¨¡æ¿:", ["è‡ªè¨‚"] + list(quick_prompts.keys()))
        
        if selected_template != "è‡ªè¨‚":
            prompt = quick_prompts[selected_template]
        
        # ç”Ÿæˆæ§åˆ¶
        col_gen1, col_gen2, col_gen3 = st.columns([2, 1, 1])
        
        with col_gen1:
            generate_btn = st.button(
                "ğŸ¨ ç”Ÿæˆåœ–åƒ",
                type="primary",
                use_container_width=True,
                disabled=not prompt.strip()
            )
        
        with col_gen2:
            if st.button("ğŸ² éš¨æ©Ÿ", use_container_width=True):
                import random
                prompt = random.choice(list(quick_prompts.values()))
                st.rerun()
        
        with col_gen3:
            if selected_service == "æ¼”ç¤ºæ¨¡å¼":
                est_time = "å³æ™‚"
            elif selected_service in API_SERVICES:
                est_time = API_SERVICES[selected_service]["avg_time"]
            else:
                est_time = "10-30ç§’"
            
            st.metric("é ä¼°æ™‚é–“", est_time)
        
        # åœ–åƒç”Ÿæˆé‚è¼¯
        if generate_btn and prompt.strip():
            # æª¢æŸ¥ API Token (æ¼”ç¤ºæ¨¡å¼é™¤å¤–)
            if selected_service != "æ¼”ç¤ºæ¨¡å¼" and not api_token:
                st.error(f"è«‹è¼¸å…¥ {selected_service} çš„ API Token")
            else:
                with st.spinner(f"ä½¿ç”¨ {selected_service} ç”Ÿæˆä¸­..."):
                    start_time = time.time()
                    
                    # èª¿ç”¨ç›¸æ‡‰çš„ç”Ÿæˆæœå‹™
                    if selected_service == "æ¼”ç¤ºæ¨¡å¼":
                        result = simulate_demo_generation(prompt)
                    elif selected_service == "Hugging Face Inference":
                        result = call_huggingface_api(prompt, api_token)
                    elif selected_service == "Replicate":
                        result = call_replicate_api(prompt, api_token)
                    else:
                        result = {"status": "error", "message": "æœå‹™æš«æœªå¯¦ç¾"}
                    
                    generation_time = time.time() - start_time
                    
                    if result["status"] == "success":
                        st.success(f"âœ… ç”ŸæˆæˆåŠŸï¼è€—æ™‚: {generation_time:.1f}ç§’")
                        
                        # é¡¯ç¤ºåœ–åƒ
                        image = result["image"]
                        
                        # åœ–åƒå£“ç¸® (å¦‚æœå•Ÿç”¨)
                        if compress_images and selected_service != "æ¼”ç¤ºæ¨¡å¼":
                            # å£“ç¸®åœ–åƒä»¥ç¯€çœå…§å­˜
                            image = image.resize((512, 512), Image.Resampling.LANCZOS)
                        
                        st.image(
                            image,
                            caption=f"æç¤ºè©: {prompt} | æœå‹™: {result.get('service', selected_service)}",
                            use_column_width=True
                        )
                        
                        # ä¸‹è¼‰åŠŸèƒ½
                        img_buffer = BytesIO()
                        image.save(img_buffer, format="PNG", optimize=True)
                        img_buffer.seek(0)
                        
                        st.download_button(
                            "ğŸ“¥ ä¸‹è¼‰åœ–åƒ",
                            data=img_buffer,
                            file_name=f"flux_koyeb_{int(time.time())}.png",
                            mime="image/png"
                        )
                        
                        # ç·©å­˜çµæœ (å¦‚æœå•Ÿç”¨)
                        if enable_cache:
                            if 'generated_cache' not in st.session_state:
                                st.session_state.generated_cache = []
                            
                            st.session_state.generated_cache.append({
                                'prompt': prompt,
                                'service': selected_service,
                                'time': time.strftime('%H:%M:%S'),
                                'generation_time': f"{generation_time:.1f}s"
                            })
                            
                            # é™åˆ¶ç·©å­˜å¤§å°
                            if len(st.session_state.generated_cache) > 5:
                                st.session_state.generated_cache.pop(0)
                    
                    else:
                        st.error(f"âŒ ç”Ÿæˆå¤±æ•—: {result['message']}")
                        
                        # æä¾›è§£æ±ºæ–¹æ¡ˆ
                        st.info("""
                        **å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:**
                        - æª¢æŸ¥ API Token æ˜¯å¦æ­£ç¢º
                        - å˜—è©¦åˆ‡æ›æ¼”ç¤ºæ¨¡å¼æ¸¬è©¦
                        - ç¢ºèªç¶²çµ¡é€£æ¥æ­£å¸¸
                        - è¯ç¹« API æœå‹™æä¾›å•†
                        """)
    
    with col2:
        st.subheader("ğŸ’¡ Koyeb å„ªå‹¢")
        
        st.markdown("""
        **ğŸš€ Koyeb ç‰¹è‰²:**
        - å…¨çƒ 50+ åœ°å€éƒ¨ç½²
        - è‡ªå‹•ç¸®æ”¾ & Scale-to-Zero  
        - å…§å»ºè² è¼‰å‡è¡¡
        - è‡ªå‹• HTTPS & SSL
        - Git é©…å‹•éƒ¨ç½²
        
        **ğŸ’° æˆæœ¬å„ªåŒ–:**
        - å…è²»å¯¦ä¾‹: $0.00/æœˆ
        - æŒ‰éœ€ä»˜è²»: $0.0036/å°æ™‚èµ·
        - ç„¡é–’ç½®è²»ç”¨ (Scale-to-Zero)
        - ç„¡åŸºç¤è¨­æ–½ç®¡ç†
        
        **ğŸ“ˆ æ€§èƒ½ç›£æ§:**
        """)
        
        # é¡¯ç¤ºç•¶å‰ç³»çµ±ç‹€æ…‹
        if "error" not in system_info:
            col_cpu, col_mem = st.columns(2)
            with col_cpu:
                st.metric("CPU ä½¿ç”¨", f"{system_info['cpu_percent']:.1f}%")
            with col_mem:
                st.metric("å…§å­˜ä½¿ç”¨", f"{system_info['memory_percent']:.1f}%")
        
        # ç”Ÿæˆæ­·å² (å¦‚æœæœ‰ç·©å­˜)
        if 'generated_cache' in st.session_state and st.session_state.generated_cache:
            st.subheader("ğŸ“š ç”Ÿæˆæ­·å²")
            
            for i, item in enumerate(reversed(st.session_state.generated_cache)):
                with st.expander(f"è¨˜éŒ„ {i+1} - {item['time']}"):
                    st.write(f"**æç¤ºè©**: {item['prompt'][:50]}...")
                    st.write(f"**æœå‹™**: {item['service']}")
                    st.write(f"**è€—æ™‚**: {item['generation_time']}")
        
        # éƒ¨ç½²æŒ‡å—
        st.subheader("ğŸ› ï¸ éƒ¨ç½²æŒ‡å—")
        
        with st.expander("ğŸ“– å¿«é€Ÿéƒ¨ç½²"):
            st.code("""
# 1. æ¨é€ä»£ç¢¼åˆ° GitHub
git init
git add .
git commit -m "Flux AI Koyeb"
git push origin main

# 2. åœ¨ Koyeb æ§åˆ¶å°
# - é»æ“Š "Create Service"
# - é¸æ“‡ GitHub å€‰åº«
# - é¸æ“‡ CPU å¯¦ä¾‹é¡å‹
# - è¨­ç½®ç’°å¢ƒè®Šé‡
# - é»æ“Š Deploy
            """, language="bash")

if __name__ == "__main__":
    main()
